# Test configuration for debugging - short training
model:
  K: 4
  r: 5
  d: 64
  backbone_name: "resnet18"
  num_classes: 80
  pretrained: true
  lambda_pac: 1.0
  eta_soc: 0.1
  alpha: 0.2
  beta: 0.05
  gamma: 0.1
  top_m_candidates: 32
  epsilon: 1.0e-3  # Increased for numerical stability
  tau: 1.0e-3      # Increased for numerical stability
  use_mahalanobis_warmup: true
  warmup_epochs: 3
  use_weighted_ce: true
  use_semi_hard: true
  use_class_proto: true
  proto_momentum: 0.9

training:
  epochs: 100 
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 1.0e-4
  optimizer: "adamw"
  scheduler: "cosine"
  warmup_lr: 1.0e-6
  min_lr: 1.0e-6
  augmentation:
    resize: 256
    crop_size: 224
    horizontal_flip: 0.5
    color_jitter: 0.1
    random_erase: 0.1
  mixed_precision: true
  channels_last: true
  compile: false
  save_frequency: 1
  log_frequency: 10
  validate_frequency: 1

dataset:
  name: "cotton80"
  root: "./data"
  train_split: "train"
  val_split: "val"
  test_split: "test"
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  num_workers: 4
  pin_memory: true
  persistent_workers: true

evaluation:
  metrics: ["accuracy", "top5_accuracy", "f1_score"]
  save_predictions: false

optimization:
  high_priority_optimizations:
    vectorized_pac: true
    two_stage_negatives: true
    frobenius_soc: true
    small_dimensions: true
    limited_negatives_pool: true
  system_optimizations:
    mixed_precision: true
    channels_last: true
    torch_compile: false
    gradient_checkpointing: false
  profiling:
    enabled: false
    log_memory: false
    trace_activities: []

experiment:
  name: "paco2_test"
  tags: ["test", "debug"]
  notes: "Test run for debugging"

debug:
  enabled: false
  profile_forward: false
  profile_backward: false
  check_gradients: false
  print_model_info: true
